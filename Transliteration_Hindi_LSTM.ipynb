{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Oyf54w_M04",
        "outputId": "cba70576-fad3-4a8f-aab6-f19ed17d3659"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aabhaa\\tआभा\\n',\n",
              " 'aabheer\\tआभीर\\n',\n",
              " 'aabhijaat\\tआभिजात\\n',\n",
              " 'aabid\\tआबिद\\n',\n",
              " 'aabshar\\tआबशर\\n',\n",
              " 'aachaaryanandan\\tआचार्यनंदन\\n',\n",
              " 'aachaaryanandanaa\\tआचार्यनंदना\\n',\n",
              " 'aachaaryasut\\tआचार्यसुत\\n',\n",
              " 'aachaaryasuta\\tआचार्यसुता\\n',\n",
              " 'aachaaryasutaa\\tआचार्यसुता\\n',\n",
              " 'aachaarynandan\\tआचार्यनंदन\\n',\n",
              " 'aachaarysuta\\tआचार्यसुता\\n',\n",
              " 'aachaarysutaa\\tआचार्यसुता\\n',\n",
              " 'aacharya\\tआचार्य\\n',\n",
              " 'aacharyanandana\\tआचार्यनंदना\\n',\n",
              " 'aacharyanandanaa\\tआचार्यनंदना\\n',\n",
              " 'aacharyasuta\\tआचार्यसुता\\n',\n",
              " 'aacharyasutaa\\tआचार्यसुता\\n',\n",
              " 'aacharynandana\\tआचार्यनंदना\\n',\n",
              " 'aacharysut\\tआचार्यसुत\\n',\n",
              " 'aachrekar\\tआचरेकर\\n',\n",
              " 'aachwal\\tआचवल\\n',\n",
              " 'aada\\tआडा\\n',\n",
              " 'aadarsh\\tआदर्श\\n',\n",
              " 'aadarsha\\tआदर्श\\n',\n",
              " 'animish\\tअनिमिश\\n',\n",
              " 'anindita\\tअनिन्दिता\\n',\n",
              " 'aninditaa\\tअनिन्दिता\\n',\n",
              " 'aniruddha\\tअनिरुद्ध\\n',\n",
              " 'anirudha\\tअनिरुद्ध\\n',\n",
              " 'anirvan\\tअनिर्वाण\\n',\n",
              " 'anisa\\tअनीसा\\n',\n",
              " 'anise\\tएनीज\\n',\n",
              " 'anisha\\tअनिशा\\n',\n",
              " 'anisur rahman\\tअनीसुर रहमान\\n',\n",
              " 'anita\\tअनिता\\n',\n",
              " 'anitaa\\tअनिता\\n',\n",
              " 'aniteja\\tअनितेज\\n',\n",
              " 'anitra\\tअनित्रा\\n',\n",
              " 'anitraa\\tअनित्रा\\n',\n",
              " 'anjaam\\tअंजाम\\n',\n",
              " 'anjaam khuda jane\\tअंजाम खुदा जाने\\n',\n",
              " 'anjaan\\tअंजान\\n',\n",
              " 'anjaane\\tअंजाने\\n',\n",
              " 'anjalee\\tअंजली\\n',\n",
              " 'anjali\\tअंजलि\\n',\n",
              " 'anjana\\tअंजना\\n',\n",
              " 'anjane mein\\tअंजाने में\\n',\n",
              " 'anjanee\\tअंजनी\\n',\n",
              " 'anjani\\tअंजनी\\n',\n",
              " 'anjar\\tअंजर\\n',\n",
              " 'anjasa\\tअंजसा\\n',\n",
              " 'anjasaa\\tअंजसा\\n',\n",
              " 'anjhi\\tअंझी\\n',\n",
              " 'anju\\tअंजु\\n',\n",
              " 'anjushri\\tअंजुश्री\\n',\n",
              " 'ankai\\tअंकाई\\n',\n",
              " 'ankh ka nasha\\tआँख का नशा\\n',\n",
              " 'ankhiyon ke jharokhon se\\tअँखियों के झरोखों से\\n',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "batch_size = 32  # Batch size for training.\n",
        "epochs = 200  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 12937  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = r'Transliteration\\NEWS2018_DATASET_04/NEWS2018_M-EnHi_trn.txt'\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "lines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSUQtTg5_M06",
        "outputId": "64cd71a4-81dd-4eed-bc0d-6fc11f390b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 12937\n",
            "Number of unique input tokens: 43\n",
            "Number of unique output tokens: 85\n",
            "Max sequence length for inputs: 61\n",
            "Max sequence length for outputs: 64\n",
            "[' ', \"'\", '(', ')', ',', '-', '.', '/', '1', '2', '4', '6', '?', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'è', 'é']\n",
            "['\\t', '\\n', ' ', \"'\", '(', ')', ',', '-', '.', '/', '1', '2', '4', '6', ':', '?', 'ँ', 'ं', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', '॥', '\\u200d']\n"
          ]
        }
      ],
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "print(input_characters) \n",
        "print(target_characters) \n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "MuAeq9iV_M08"
      },
      "outputs": [],
      "source": [
        "#Build the model\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1kQ688z_M09",
        "outputId": "9b8d0707-0bcf-42d9-fdb7-0adece939c4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "364/364 [==============================] - 14s 17ms/step - loss: 0.6390 - accuracy: 0.8602 - val_loss: 0.5576 - val_accuracy: 0.8624\n",
            "Epoch 2/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.4640 - accuracy: 0.8758 - val_loss: 0.5139 - val_accuracy: 0.8678\n",
            "Epoch 3/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.4360 - accuracy: 0.8803 - val_loss: 0.4964 - val_accuracy: 0.8732\n",
            "Epoch 4/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.4214 - accuracy: 0.8827 - val_loss: 0.4912 - val_accuracy: 0.8746\n",
            "Epoch 5/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.4109 - accuracy: 0.8847 - val_loss: 0.4826 - val_accuracy: 0.8768\n",
            "Epoch 6/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.4003 - accuracy: 0.8875 - val_loss: 0.4760 - val_accuracy: 0.8788\n",
            "Epoch 7/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.3910 - accuracy: 0.8897 - val_loss: 0.4724 - val_accuracy: 0.8803\n",
            "Epoch 8/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.3831 - accuracy: 0.8919 - val_loss: 0.4709 - val_accuracy: 0.8814\n",
            "Epoch 9/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.3761 - accuracy: 0.8937 - val_loss: 0.4640 - val_accuracy: 0.8837\n",
            "Epoch 10/200\n",
            "364/364 [==============================] - 5s 12ms/step - loss: 0.3699 - accuracy: 0.8953 - val_loss: 0.4581 - val_accuracy: 0.8847\n",
            "Epoch 11/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.3633 - accuracy: 0.8967 - val_loss: 0.4593 - val_accuracy: 0.8840\n",
            "Epoch 12/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.3576 - accuracy: 0.8982 - val_loss: 0.4552 - val_accuracy: 0.8857\n",
            "Epoch 13/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.3520 - accuracy: 0.8996 - val_loss: 0.4544 - val_accuracy: 0.8883\n",
            "Epoch 14/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.3463 - accuracy: 0.9009 - val_loss: 0.4600 - val_accuracy: 0.8855\n",
            "Epoch 15/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.3407 - accuracy: 0.9024 - val_loss: 0.4509 - val_accuracy: 0.8899\n",
            "Epoch 16/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.3278 - accuracy: 0.9057 - val_loss: 0.4462 - val_accuracy: 0.8916\n",
            "Epoch 17/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.3136 - accuracy: 0.9101 - val_loss: 0.4423 - val_accuracy: 0.8908\n",
            "Epoch 18/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.2963 - accuracy: 0.9147 - val_loss: 0.4300 - val_accuracy: 0.8957\n",
            "Epoch 19/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.2808 - accuracy: 0.9192 - val_loss: 0.4281 - val_accuracy: 0.8965\n",
            "Epoch 20/200\n",
            "364/364 [==============================] - 5s 15ms/step - loss: 0.2685 - accuracy: 0.9226 - val_loss: 0.4312 - val_accuracy: 0.8983\n",
            "Epoch 21/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.2591 - accuracy: 0.9254 - val_loss: 0.4281 - val_accuracy: 0.8996\n",
            "Epoch 22/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.2508 - accuracy: 0.9274 - val_loss: 0.4189 - val_accuracy: 0.8991\n",
            "Epoch 23/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.2427 - accuracy: 0.9297 - val_loss: 0.4190 - val_accuracy: 0.9012\n",
            "Epoch 24/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.2346 - accuracy: 0.9318 - val_loss: 0.4143 - val_accuracy: 0.9026\n",
            "Epoch 25/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.2267 - accuracy: 0.9337 - val_loss: 0.4100 - val_accuracy: 0.9032\n",
            "Epoch 26/200\n",
            "364/364 [==============================] - 5s 15ms/step - loss: 0.2194 - accuracy: 0.9357 - val_loss: 0.4138 - val_accuracy: 0.9025\n",
            "Epoch 27/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.2130 - accuracy: 0.9376 - val_loss: 0.4193 - val_accuracy: 0.9026\n",
            "Epoch 28/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.2067 - accuracy: 0.9392 - val_loss: 0.4195 - val_accuracy: 0.9039\n",
            "Epoch 29/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.2005 - accuracy: 0.9410 - val_loss: 0.4202 - val_accuracy: 0.9025\n",
            "Epoch 30/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.1945 - accuracy: 0.9428 - val_loss: 0.4137 - val_accuracy: 0.9030\n",
            "Epoch 31/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.1888 - accuracy: 0.9442 - val_loss: 0.4203 - val_accuracy: 0.9034\n",
            "Epoch 32/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.1826 - accuracy: 0.9462 - val_loss: 0.4204 - val_accuracy: 0.9047\n",
            "Epoch 33/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.1759 - accuracy: 0.9477 - val_loss: 0.4211 - val_accuracy: 0.9031\n",
            "Epoch 34/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.1702 - accuracy: 0.9493 - val_loss: 0.4305 - val_accuracy: 0.9018\n",
            "Epoch 35/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.1651 - accuracy: 0.9506 - val_loss: 0.4322 - val_accuracy: 0.9029\n",
            "Epoch 36/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.1588 - accuracy: 0.9528 - val_loss: 0.4294 - val_accuracy: 0.9034\n",
            "Epoch 37/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.1541 - accuracy: 0.9538 - val_loss: 0.4308 - val_accuracy: 0.9032\n",
            "Epoch 38/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.1484 - accuracy: 0.9555 - val_loss: 0.4439 - val_accuracy: 0.9021\n",
            "Epoch 39/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.1439 - accuracy: 0.9569 - val_loss: 0.4460 - val_accuracy: 0.9017\n",
            "Epoch 40/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.1393 - accuracy: 0.9583 - val_loss: 0.4445 - val_accuracy: 0.9034\n",
            "Epoch 41/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.1356 - accuracy: 0.9591 - val_loss: 0.4520 - val_accuracy: 0.9013\n",
            "Epoch 42/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.1302 - accuracy: 0.9608 - val_loss: 0.4553 - val_accuracy: 0.9005\n",
            "Epoch 43/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.1266 - accuracy: 0.9620 - val_loss: 0.4635 - val_accuracy: 0.9013\n",
            "Epoch 44/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.1225 - accuracy: 0.9633 - val_loss: 0.4595 - val_accuracy: 0.9021\n",
            "Epoch 45/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.1183 - accuracy: 0.9642 - val_loss: 0.4779 - val_accuracy: 0.9023\n",
            "Epoch 46/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.1145 - accuracy: 0.9656 - val_loss: 0.4787 - val_accuracy: 0.9002\n",
            "Epoch 47/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.1154 - accuracy: 0.9651 - val_loss: 0.4727 - val_accuracy: 0.9007\n",
            "Epoch 48/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.1095 - accuracy: 0.9668 - val_loss: 0.4890 - val_accuracy: 0.9011\n",
            "Epoch 49/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.1036 - accuracy: 0.9689 - val_loss: 0.4896 - val_accuracy: 0.9007\n",
            "Epoch 50/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0998 - accuracy: 0.9701 - val_loss: 0.4946 - val_accuracy: 0.9008\n",
            "Epoch 51/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0979 - accuracy: 0.9705 - val_loss: 0.5017 - val_accuracy: 0.9003\n",
            "Epoch 52/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0955 - accuracy: 0.9712 - val_loss: 0.5074 - val_accuracy: 0.9007\n",
            "Epoch 53/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0943 - accuracy: 0.9716 - val_loss: 0.5204 - val_accuracy: 0.9003\n",
            "Epoch 54/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0887 - accuracy: 0.9734 - val_loss: 0.5224 - val_accuracy: 0.8996\n",
            "Epoch 55/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0864 - accuracy: 0.9742 - val_loss: 0.5278 - val_accuracy: 0.8990\n",
            "Epoch 56/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0841 - accuracy: 0.9747 - val_loss: 0.5362 - val_accuracy: 0.8973\n",
            "Epoch 57/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0818 - accuracy: 0.9754 - val_loss: 0.5455 - val_accuracy: 0.8982\n",
            "Epoch 58/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0797 - accuracy: 0.9762 - val_loss: 0.5469 - val_accuracy: 0.8979\n",
            "Epoch 59/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0781 - accuracy: 0.9764 - val_loss: 0.5429 - val_accuracy: 0.8984\n",
            "Epoch 60/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0776 - accuracy: 0.9767 - val_loss: 0.5532 - val_accuracy: 0.8982\n",
            "Epoch 61/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0723 - accuracy: 0.9786 - val_loss: 0.5577 - val_accuracy: 0.8990\n",
            "Epoch 62/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0700 - accuracy: 0.9791 - val_loss: 0.5675 - val_accuracy: 0.8979\n",
            "Epoch 63/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0687 - accuracy: 0.9794 - val_loss: 0.5861 - val_accuracy: 0.8969\n",
            "Epoch 64/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0669 - accuracy: 0.9801 - val_loss: 0.5859 - val_accuracy: 0.8971\n",
            "Epoch 65/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0655 - accuracy: 0.9803 - val_loss: 0.5793 - val_accuracy: 0.8988\n",
            "Epoch 66/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0643 - accuracy: 0.9808 - val_loss: 0.5934 - val_accuracy: 0.8965\n",
            "Epoch 67/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0619 - accuracy: 0.9815 - val_loss: 0.5919 - val_accuracy: 0.8966\n",
            "Epoch 68/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0602 - accuracy: 0.9821 - val_loss: 0.5981 - val_accuracy: 0.8983\n",
            "Epoch 69/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0584 - accuracy: 0.9828 - val_loss: 0.6122 - val_accuracy: 0.8970\n",
            "Epoch 70/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0579 - accuracy: 0.9826 - val_loss: 0.6135 - val_accuracy: 0.8968\n",
            "Epoch 71/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0553 - accuracy: 0.9837 - val_loss: 0.6242 - val_accuracy: 0.8967\n",
            "Epoch 72/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0553 - accuracy: 0.9835 - val_loss: 0.6198 - val_accuracy: 0.8983\n",
            "Epoch 73/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0535 - accuracy: 0.9841 - val_loss: 0.6430 - val_accuracy: 0.8963\n",
            "Epoch 74/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0528 - accuracy: 0.9843 - val_loss: 0.6445 - val_accuracy: 0.8963\n",
            "Epoch 75/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0502 - accuracy: 0.9853 - val_loss: 0.6459 - val_accuracy: 0.8971\n",
            "Epoch 76/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0504 - accuracy: 0.9849 - val_loss: 0.6536 - val_accuracy: 0.8970\n",
            "Epoch 77/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0489 - accuracy: 0.9854 - val_loss: 0.6576 - val_accuracy: 0.8956\n",
            "Epoch 78/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0491 - accuracy: 0.9853 - val_loss: 0.6685 - val_accuracy: 0.8961\n",
            "Epoch 79/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0458 - accuracy: 0.9866 - val_loss: 0.6647 - val_accuracy: 0.8964\n",
            "Epoch 80/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0445 - accuracy: 0.9869 - val_loss: 0.6797 - val_accuracy: 0.8947\n",
            "Epoch 81/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0443 - accuracy: 0.9869 - val_loss: 0.6807 - val_accuracy: 0.8953\n",
            "Epoch 82/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0929 - accuracy: 0.9717 - val_loss: 0.6361 - val_accuracy: 0.8970\n",
            "Epoch 83/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0519 - accuracy: 0.9842 - val_loss: 0.6559 - val_accuracy: 0.8957\n",
            "Epoch 84/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0419 - accuracy: 0.9880 - val_loss: 0.6695 - val_accuracy: 0.8964\n",
            "Epoch 85/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0388 - accuracy: 0.9890 - val_loss: 0.6867 - val_accuracy: 0.8959\n",
            "Epoch 86/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0376 - accuracy: 0.9892 - val_loss: 0.6884 - val_accuracy: 0.8969\n",
            "Epoch 87/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0385 - accuracy: 0.9888 - val_loss: 0.7135 - val_accuracy: 0.8941\n",
            "Epoch 88/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0402 - accuracy: 0.9880 - val_loss: 0.6965 - val_accuracy: 0.8967\n",
            "Epoch 89/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0430 - accuracy: 0.9868 - val_loss: 0.7098 - val_accuracy: 0.8945\n",
            "Epoch 90/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0393 - accuracy: 0.9883 - val_loss: 0.7156 - val_accuracy: 0.8946\n",
            "Epoch 91/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0348 - accuracy: 0.9901 - val_loss: 0.7203 - val_accuracy: 0.8954\n",
            "Epoch 92/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0344 - accuracy: 0.9901 - val_loss: 0.7260 - val_accuracy: 0.8953\n",
            "Epoch 93/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0347 - accuracy: 0.9898 - val_loss: 0.7263 - val_accuracy: 0.8952\n",
            "Epoch 94/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0372 - accuracy: 0.9889 - val_loss: 0.7354 - val_accuracy: 0.8959\n",
            "Epoch 95/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0338 - accuracy: 0.9902 - val_loss: 0.7450 - val_accuracy: 0.8963\n",
            "Epoch 96/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0340 - accuracy: 0.9899 - val_loss: 0.7483 - val_accuracy: 0.8945\n",
            "Epoch 97/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0358 - accuracy: 0.9891 - val_loss: 0.7465 - val_accuracy: 0.8947\n",
            "Epoch 98/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0332 - accuracy: 0.9902 - val_loss: 0.7437 - val_accuracy: 0.8966\n",
            "Epoch 99/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0314 - accuracy: 0.9909 - val_loss: 0.7609 - val_accuracy: 0.8937\n",
            "Epoch 100/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0307 - accuracy: 0.9910 - val_loss: 0.7690 - val_accuracy: 0.8945\n",
            "Epoch 101/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0304 - accuracy: 0.9910 - val_loss: 0.7772 - val_accuracy: 0.8943\n",
            "Epoch 102/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0317 - accuracy: 0.9905 - val_loss: 0.7843 - val_accuracy: 0.8934\n",
            "Epoch 103/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0324 - accuracy: 0.9901 - val_loss: 0.7924 - val_accuracy: 0.8933\n",
            "Epoch 104/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0310 - accuracy: 0.9907 - val_loss: 0.7775 - val_accuracy: 0.8955\n",
            "Epoch 105/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0276 - accuracy: 0.9920 - val_loss: 0.8001 - val_accuracy: 0.8935\n",
            "Epoch 106/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0317 - accuracy: 0.9903 - val_loss: 0.7781 - val_accuracy: 0.8956\n",
            "Epoch 107/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0305 - accuracy: 0.9909 - val_loss: 0.8000 - val_accuracy: 0.8940\n",
            "Epoch 108/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0291 - accuracy: 0.9914 - val_loss: 0.7957 - val_accuracy: 0.8943\n",
            "Epoch 109/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0263 - accuracy: 0.9923 - val_loss: 0.8089 - val_accuracy: 0.8940\n",
            "Epoch 110/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0239 - accuracy: 0.9931 - val_loss: 0.7965 - val_accuracy: 0.8951\n",
            "Epoch 111/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0238 - accuracy: 0.9932 - val_loss: 0.8441 - val_accuracy: 0.8913\n",
            "Epoch 112/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0275 - accuracy: 0.9917 - val_loss: 0.8220 - val_accuracy: 0.8935\n",
            "Epoch 113/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0318 - accuracy: 0.9901 - val_loss: 0.8144 - val_accuracy: 0.8939\n",
            "Epoch 114/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0269 - accuracy: 0.9920 - val_loss: 0.8270 - val_accuracy: 0.8930\n",
            "Epoch 115/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0228 - accuracy: 0.9934 - val_loss: 0.8354 - val_accuracy: 0.8937\n",
            "Epoch 116/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0232 - accuracy: 0.9934 - val_loss: 0.8476 - val_accuracy: 0.8933\n",
            "Epoch 117/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0226 - accuracy: 0.9935 - val_loss: 0.8642 - val_accuracy: 0.8924\n",
            "Epoch 118/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0236 - accuracy: 0.9930 - val_loss: 0.8508 - val_accuracy: 0.8934\n",
            "Epoch 119/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0256 - accuracy: 0.9922 - val_loss: 0.8547 - val_accuracy: 0.8945\n",
            "Epoch 120/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0254 - accuracy: 0.9923 - val_loss: 0.8428 - val_accuracy: 0.8934\n",
            "Epoch 121/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0263 - accuracy: 0.9920 - val_loss: 0.8501 - val_accuracy: 0.8943\n",
            "Epoch 122/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0227 - accuracy: 0.9932 - val_loss: 0.8771 - val_accuracy: 0.8922\n",
            "Epoch 123/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0189 - accuracy: 0.9948 - val_loss: 0.8588 - val_accuracy: 0.8940\n",
            "Epoch 124/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0213 - accuracy: 0.9938 - val_loss: 0.8769 - val_accuracy: 0.8924\n",
            "Epoch 125/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0258 - accuracy: 0.9920 - val_loss: 0.8703 - val_accuracy: 0.8948\n",
            "Epoch 126/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0204 - accuracy: 0.9941 - val_loss: 0.8793 - val_accuracy: 0.8937\n",
            "Epoch 127/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.8868 - val_accuracy: 0.8931\n",
            "Epoch 128/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0212 - accuracy: 0.9938 - val_loss: 0.9177 - val_accuracy: 0.8921\n",
            "Epoch 129/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0237 - accuracy: 0.9928 - val_loss: 0.8929 - val_accuracy: 0.8921\n",
            "Epoch 130/200\n",
            "364/364 [==============================] - 5s 15ms/step - loss: 0.0173 - accuracy: 0.9952 - val_loss: 0.9023 - val_accuracy: 0.8934\n",
            "Epoch 131/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0195 - accuracy: 0.9943 - val_loss: 0.9094 - val_accuracy: 0.8930\n",
            "Epoch 132/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0217 - accuracy: 0.9936 - val_loss: 0.8974 - val_accuracy: 0.8920\n",
            "Epoch 133/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0231 - accuracy: 0.9929 - val_loss: 0.9184 - val_accuracy: 0.8918\n",
            "Epoch 134/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0181 - accuracy: 0.9950 - val_loss: 0.9077 - val_accuracy: 0.8930\n",
            "Epoch 135/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0180 - accuracy: 0.9949 - val_loss: 0.9100 - val_accuracy: 0.8940\n",
            "Epoch 136/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0162 - accuracy: 0.9956 - val_loss: 0.9188 - val_accuracy: 0.8934\n",
            "Epoch 137/200\n",
            "364/364 [==============================] - 5s 12ms/step - loss: 0.0242 - accuracy: 0.9925 - val_loss: 0.9224 - val_accuracy: 0.8926\n",
            "Epoch 138/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0230 - accuracy: 0.9928 - val_loss: 0.9046 - val_accuracy: 0.8937\n",
            "Epoch 139/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0162 - accuracy: 0.9955 - val_loss: 0.9453 - val_accuracy: 0.8924\n",
            "Epoch 140/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0169 - accuracy: 0.9952 - val_loss: 0.9328 - val_accuracy: 0.8926\n",
            "Epoch 141/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0149 - accuracy: 0.9959 - val_loss: 0.9395 - val_accuracy: 0.8929\n",
            "Epoch 142/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0150 - accuracy: 0.9959 - val_loss: 0.9417 - val_accuracy: 0.8931\n",
            "Epoch 143/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0176 - accuracy: 0.9949 - val_loss: 0.9514 - val_accuracy: 0.8935\n",
            "Epoch 144/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0257 - accuracy: 0.9918 - val_loss: 0.9490 - val_accuracy: 0.8929\n",
            "Epoch 145/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0162 - accuracy: 0.9955 - val_loss: 0.9466 - val_accuracy: 0.8920\n",
            "Epoch 146/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0144 - accuracy: 0.9961 - val_loss: 0.9631 - val_accuracy: 0.8923\n",
            "Epoch 147/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0147 - accuracy: 0.9959 - val_loss: 0.9550 - val_accuracy: 0.8933\n",
            "Epoch 148/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0187 - accuracy: 0.9944 - val_loss: 0.9506 - val_accuracy: 0.8928\n",
            "Epoch 149/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0150 - accuracy: 0.9958 - val_loss: 0.9663 - val_accuracy: 0.8927\n",
            "Epoch 150/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.9683 - val_accuracy: 0.8934\n",
            "Epoch 151/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0141 - accuracy: 0.9961 - val_loss: 0.9708 - val_accuracy: 0.8935\n",
            "Epoch 152/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0156 - accuracy: 0.9955 - val_loss: 0.9714 - val_accuracy: 0.8928\n",
            "Epoch 153/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0193 - accuracy: 0.9940 - val_loss: 0.9873 - val_accuracy: 0.8928\n",
            "Epoch 154/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0158 - accuracy: 0.9953 - val_loss: 0.9914 - val_accuracy: 0.8933\n",
            "Epoch 155/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0152 - accuracy: 0.9957 - val_loss: 0.9812 - val_accuracy: 0.8930\n",
            "Epoch 156/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0191 - accuracy: 0.9942 - val_loss: 0.9787 - val_accuracy: 0.8936\n",
            "Epoch 157/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0145 - accuracy: 0.9959 - val_loss: 1.0003 - val_accuracy: 0.8928\n",
            "Epoch 158/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0129 - accuracy: 0.9965 - val_loss: 0.9882 - val_accuracy: 0.8934\n",
            "Epoch 159/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0121 - accuracy: 0.9968 - val_loss: 0.9895 - val_accuracy: 0.8930\n",
            "Epoch 160/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0122 - accuracy: 0.9967 - val_loss: 0.9939 - val_accuracy: 0.8922\n",
            "Epoch 161/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.9955 - val_accuracy: 0.8923\n",
            "Epoch 162/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0191 - accuracy: 0.9941 - val_loss: 0.9912 - val_accuracy: 0.8921\n",
            "Epoch 163/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0162 - accuracy: 0.9951 - val_loss: 1.0000 - val_accuracy: 0.8929\n",
            "Epoch 164/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0114 - accuracy: 0.9970 - val_loss: 1.0016 - val_accuracy: 0.8924\n",
            "Epoch 165/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0095 - accuracy: 0.9977 - val_loss: 1.0164 - val_accuracy: 0.8926\n",
            "Epoch 166/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0103 - accuracy: 0.9973 - val_loss: 1.0338 - val_accuracy: 0.8928\n",
            "Epoch 167/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0407 - accuracy: 0.9884 - val_loss: 1.0053 - val_accuracy: 0.8896\n",
            "Epoch 168/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0406 - accuracy: 0.9877 - val_loss: 0.9968 - val_accuracy: 0.8930\n",
            "Epoch 169/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0144 - accuracy: 0.9963 - val_loss: 1.0067 - val_accuracy: 0.8932\n",
            "Epoch 170/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0094 - accuracy: 0.9979 - val_loss: 1.0226 - val_accuracy: 0.8927\n",
            "Epoch 171/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0084 - accuracy: 0.9982 - val_loss: 1.0193 - val_accuracy: 0.8929\n",
            "Epoch 172/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0068 - accuracy: 0.9986 - val_loss: 1.0251 - val_accuracy: 0.8933\n",
            "Epoch 173/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0061 - accuracy: 0.9988 - val_loss: 1.0489 - val_accuracy: 0.8928\n",
            "Epoch 174/200\n",
            "364/364 [==============================] - 5s 13ms/step - loss: 0.0140 - accuracy: 0.9960 - val_loss: 1.0279 - val_accuracy: 0.8934\n",
            "Epoch 175/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0212 - accuracy: 0.9935 - val_loss: 1.0127 - val_accuracy: 0.8938\n",
            "Epoch 176/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0142 - accuracy: 0.9958 - val_loss: 1.0284 - val_accuracy: 0.8932\n",
            "Epoch 177/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0100 - accuracy: 0.9975 - val_loss: 1.0373 - val_accuracy: 0.8923\n",
            "Epoch 178/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0083 - accuracy: 0.9981 - val_loss: 1.0529 - val_accuracy: 0.8921\n",
            "Epoch 179/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0099 - accuracy: 0.9974 - val_loss: 1.0367 - val_accuracy: 0.8938\n",
            "Epoch 180/200\n",
            "364/364 [==============================] - 5s 15ms/step - loss: 0.0184 - accuracy: 0.9943 - val_loss: 1.0349 - val_accuracy: 0.8922\n",
            "Epoch 181/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0163 - accuracy: 0.9950 - val_loss: 1.0490 - val_accuracy: 0.8926\n",
            "Epoch 182/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0109 - accuracy: 0.9972 - val_loss: 1.0436 - val_accuracy: 0.8937\n",
            "Epoch 183/200\n",
            "364/364 [==============================] - 5s 15ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 1.0596 - val_accuracy: 0.8922\n",
            "Epoch 184/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0090 - accuracy: 0.9978 - val_loss: 1.0567 - val_accuracy: 0.8919\n",
            "Epoch 185/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 1.0645 - val_accuracy: 0.8938\n",
            "Epoch 186/200\n",
            "364/364 [==============================] - 5s 15ms/step - loss: 0.0131 - accuracy: 0.9962 - val_loss: 1.0638 - val_accuracy: 0.8928\n",
            "Epoch 187/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0159 - accuracy: 0.9952 - val_loss: 1.0598 - val_accuracy: 0.8932\n",
            "Epoch 188/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0144 - accuracy: 0.9957 - val_loss: 1.0708 - val_accuracy: 0.8926\n",
            "Epoch 189/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0105 - accuracy: 0.9972 - val_loss: 1.0733 - val_accuracy: 0.8927\n",
            "Epoch 190/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 1.0805 - val_accuracy: 0.8928\n",
            "Epoch 191/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0089 - accuracy: 0.9977 - val_loss: 1.0918 - val_accuracy: 0.8922\n",
            "Epoch 192/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0144 - accuracy: 0.9956 - val_loss: 1.0716 - val_accuracy: 0.8924\n",
            "Epoch 193/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0131 - accuracy: 0.9962 - val_loss: 1.0772 - val_accuracy: 0.8929\n",
            "Epoch 194/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0098 - accuracy: 0.9975 - val_loss: 1.0925 - val_accuracy: 0.8928\n",
            "Epoch 195/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0101 - accuracy: 0.9973 - val_loss: 1.0870 - val_accuracy: 0.8926\n",
            "Epoch 196/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 1.1088 - val_accuracy: 0.8917\n",
            "Epoch 197/200\n",
            "364/364 [==============================] - 4s 12ms/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 1.1025 - val_accuracy: 0.8918\n",
            "Epoch 198/200\n",
            "364/364 [==============================] - 5s 14ms/step - loss: 0.0132 - accuracy: 0.9961 - val_loss: 1.1354 - val_accuracy: 0.8912\n",
            "Epoch 199/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 1.0774 - val_accuracy: 0.8930\n",
            "Epoch 200/200\n",
            "364/364 [==============================] - 4s 11ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 1.0676 - val_accuracy: 0.8941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.1,\n",
        ")\n",
        "# Save model\n",
        "model.save(r'/content/drive/MyDrive/Colab Notebooks/Transliteration/s2s_google_1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "JbbWWEXe_M0-"
      },
      "outputs": [],
      "source": [
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(r'/content/drive/MyDrive/Colab Notebooks/Transliteration/s2s_google_1')\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip show tensorflow"
      ],
      "metadata": {
        "id": "lMfbmGI-kw6r"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "66yUrXeZ_M1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08cf86b1-5aaa-4ad1-b468-c72e01ad9164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 314ms/step\n",
            "1/1 [==============================] - 0s 321ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "-\n",
            "Input sentence: aabhaa\n",
            "Decoded sentence: आभा\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-\n",
            "Input sentence: aabheer\n",
            "Decoded sentence: आभीर\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-\n",
            "Input sentence: aabhijaat\n",
            "Decoded sentence: आभिजात\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-\n",
            "Input sentence: aabid\n",
            "Decoded sentence: आबिद\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "-\n",
            "Input sentence: aabshar\n",
            "Decoded sentence: आबशर\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "-\n",
            "Input sentence: aachaaryanandan\n",
            "Decoded sentence: आचार्यनंदन\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "-\n",
            "Input sentence: aachaaryanandanaa\n",
            "Decoded sentence: आचार्यनंदना\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "-\n",
            "Input sentence: aachaaryasut\n",
            "Decoded sentence: आचार्यसुत\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "-\n",
            "Input sentence: aachaaryasuta\n",
            "Decoded sentence: आचार्यसुता\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: aachaaryasutaa\n",
            "Decoded sentence: आचार्यसुता\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-\n",
            "Input sentence: aachaarynandan\n",
            "Decoded sentence: आचार्यनंदन\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "-\n",
            "Input sentence: aachaarysuta\n",
            "Decoded sentence: आचार्यसुता\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-\n",
            "Input sentence: aachaarysutaa\n",
            "Decoded sentence: आचार्यसुता\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: aacharya\n",
            "Decoded sentence: आचार्य\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-\n",
            "Input sentence: aacharyanandana\n",
            "Decoded sentence: आचार्यनंदना\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "-\n",
            "Input sentence: aacharyanandanaa\n",
            "Decoded sentence: आचार्यनंदना\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: aacharyasuta\n",
            "Decoded sentence: आचार्यसुता\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-\n",
            "Input sentence: aacharyasutaa\n",
            "Decoded sentence: आचार्यसुता\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "-\n",
            "Input sentence: aacharynandana\n",
            "Decoded sentence: आचार्यनंदना\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "-\n",
            "Input sentence: aacharysut\n",
            "Decoded sentence: आचार्यसुत\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIGcNGtx_M1E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
